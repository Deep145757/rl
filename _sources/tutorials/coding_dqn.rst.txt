
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/coding_dqn.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_coding_dqn.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_coding_dqn.py:


TorchRL trainer: A DQN example
==============================
**Author**: `Vincent Moens <https://github.com/vmoens>`_

.. GENERATED FROM PYTHON SOURCE LINES 10-84

TorchRL provides a generic :class:`~torchrl.trainers.Trainer` class to handle
your training loop. The trainer executes a nested loop where the outer loop
is the data collection and the inner loop consumes this data or some data
retrieved from the replay buffer to train the model.
At various points in this training loop, hooks can be attached and executed at
given intervals.

In this tutorial, we will be using the trainer class to train a DQN algorithm
to solve the CartPole task from scratch.

Main takeaways:

- Building a trainer with its essential components: data collector, loss
  module, replay buffer and optimizer.
- Adding hooks to a trainer, such as loggers, target network updaters and such.

The trainer is fully customisable and offers a large set of functionalities.
The tutorial is organised around its construction.
We will be detailing how to build each of the components of the library first,
and then put the pieces together using the :class:`~torchrl.trainers.Trainer`
class.

Along the road, we will also focus on some other aspects of the library:

- how to build an environment in TorchRL, including transforms (e.g. data
  normalization, frame concatenation, resizing and turning to grayscale)
  and parallel execution. Unlike what we did in the
  `DDPG tutorial <https://pytorch.org/rl/tutorials/coding_ddpg.html>`_, we
  will normalize the pixels and not the state vector.
- how to design a :class:`~torchrl.modules.QValueActor` object, i.e. an actor
  that estimates the action values and picks up the action with the highest
  estimated return;
- how to collect data from your environment efficiently and store them
  in a replay buffer;
- how to use multi-step, a simple preprocessing step for off-policy algorithms;
- and finally how to evaluate your model.

**Prerequisites**: We encourage you to get familiar with torchrl through the
`PPO tutorial <https://pytorch.org/rl/tutorials/coding_ppo.html>`_ first.

DQN
---

DQN (`Deep Q-Learning <https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf>`_) was
the founding work in deep reinforcement learning.

On a high level, the algorithm is quite simple: Q-learning consists in
learning a table of state-action values in such a way that, when
encountering any particular state, we know which action to pick just by
searching for the one with the highest value. This simple setting
requires the actions and states to be
discrete, otherwise a lookup table cannot be built.

DQN uses a neural network that encodes a map from the state-action space to
a value (scalar) space, which amortizes the cost of storing and exploring all
the possible state-action combinations: if a state has not been seen in the
past, we can still pass it in conjunction with the various actions available
through our neural network and get an interpolated value for each of the
actions available.

We will solve the classic control problem of the cart pole. From the
Gymnasium doc from where this environment is retrieved:

| A pole is attached by an un-actuated joint to a cart, which moves along a
| frictionless track. The pendulum is placed upright on the cart and the goal
| is to balance the pole by applying forces in the left and right direction
| on the cart.

.. figure:: /_static/img/cartpole_demo.gif
   :alt: Cart Pole

We do not aim at giving a SOTA implementation of the algorithm, but rather
to provide a high-level illustration of TorchRL features in the context
of this algorithm.

.. GENERATED FROM PYTHON SOURCE LINES 84-136

.. code-block:: default



    import os
    import uuid

    import torch
    from torch import nn
    from torchrl.collectors import MultiaSyncDataCollector
    from torchrl.data import LazyMemmapStorage, MultiStep, TensorDictReplayBuffer
    from torchrl.envs import (
        EnvCreator,
        ExplorationType,
        ParallelEnv,
        RewardScaling,
        StepCounter,
    )
    from torchrl.envs.libs.gym import GymEnv
    from torchrl.envs.transforms import (
        CatFrames,
        Compose,
        GrayScale,
        ObservationNorm,
        Resize,
        ToTensorImage,
        TransformedEnv,
    )
    from torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor

    from torchrl.objectives import DQNLoss, SoftUpdate
    from torchrl.record.loggers.csv import CSVLogger
    from torchrl.trainers import (
        LogReward,
        Recorder,
        ReplayBufferTrainer,
        Trainer,
        UpdateWeights,
    )


    def is_notebook() -> bool:
        try:
            shell = get_ipython().__class__.__name__
            if shell == "ZMQInteractiveShell":
                return True  # Jupyter notebook or qtconsole
            elif shell == "TerminalInteractiveShell":
                return False  # Terminal running IPython
            else:
                return False  # Other type (?)
        except NameError:
            return False  # Probably standard Python interpreter









.. GENERATED FROM PYTHON SOURCE LINES 143-195

Let's get started with the various pieces we need for our algorithm:

- An environment;
- A policy (and related modules that we group under the "model" umbrella);
- A data collector, which makes the policy play in the environment and
  delivers training data;
- A replay buffer to store the training data;
- A loss module, which computes the objective function to train our policy
  to maximise the return;
- An optimizer, which performs parameter updates based on our loss.

Additional modules include a logger, a recorder (executes the policy in
"eval" mode) and a target network updater. With all these components into
place, it is easy to see how one could misplace or misuse one component in
the training script. The trainer is there to orchestrate everything for you!

Building the environment
------------------------

First let's write a helper function that will output an environment. As usual,
the "raw" environment may be too simple to be used in practice and we'll need
some data transformation to expose its output to the policy.

We will be using five transforms:

- :class:`~torchrl.envs.StepCounter` to count the number of steps in each trajectory;
- :class:`~torchrl.envs.transforms.ToTensorImage` will convert a ``[W, H, C]`` uint8
  tensor in a floating point tensor in the ``[0, 1]`` space with shape
  ``[C, W, H]``;
- :class:`~torchrl.envs.transforms.RewardScaling` to reduce the scale of the return;
- :class:`~torchrl.envs.transforms.GrayScale` will turn our image into grayscale;
- :class:`~torchrl.envs.transforms.Resize` will resize the image in a 64x64 format;
- :class:`~torchrl.envs.transforms.CatFrames` will concatenate an arbitrary number of
  successive frames (``N=4``) in a single tensor along the channel dimension.
  This is useful as a single image does not carry information about the
  motion of the cartpole. Some memory about past observations and actions
  is needed, either via a recurrent neural network or using a stack of
  frames.
- :class:`~torchrl.envs.transforms.ObservationNorm` which will normalize our observations
  given some custom summary statistics.

In practice, our environment builder has two arguments:

- ``parallel``: determines whether multiple environments have to be run in
  parallel. We stack the transforms after the
  :class:`~torchrl.envs.ParallelEnv` to take advantage
  of vectorization of the operations on device, although this would
  technically work with every single environment attached to its own set of
  transforms.
- ``obs_norm_sd`` will contain the normalizing constants for
  the :class:`~torchrl.envs.ObservationNorm` transform.


.. GENERATED FROM PYTHON SOURCE LINES 195-238

.. code-block:: default



    def make_env(
        parallel=False,
        obs_norm_sd=None,
    ):
        if obs_norm_sd is None:
            obs_norm_sd = {"standard_normal": True}
        if parallel:
            base_env = ParallelEnv(
                num_workers,
                EnvCreator(
                    lambda: GymEnv(
                        "CartPole-v1",
                        from_pixels=True,
                        pixels_only=True,
                        device=device,
                    )
                ),
            )
        else:
            base_env = GymEnv(
                "CartPole-v1",
                from_pixels=True,
                pixels_only=True,
                device=device,
            )

        env = TransformedEnv(
            base_env,
            Compose(
                StepCounter(),  # to count the steps of each trajectory
                ToTensorImage(),
                RewardScaling(loc=0.0, scale=0.1),
                GrayScale(),
                Resize(64, 64),
                CatFrames(4, in_keys=["pixels"], dim=-3),
                ObservationNorm(in_keys=["pixels"], **obs_norm_sd),
            ),
        )
        return env









.. GENERATED FROM PYTHON SOURCE LINES 239-250

Compute normalizing constants
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To normalize images, we don't want to normalize each pixel independently
with a full ``[C, W, H]`` normalizing mask, but with simpler ``[C, 1, 1]``
shaped set of normalizing constants (loc and scale parameters).
We will be using the ``reduce_dim`` argument
of :meth:`~torchrl.envs.ObservationNorm.init_stats` to instruct which
dimensions must be reduced, and the ``keep_dims`` parameter to ensure that
not all dimensions disappear in the process:


.. GENERATED FROM PYTHON SOURCE LINES 250-264

.. code-block:: default



    def get_norm_stats():
        test_env = make_env()
        test_env.transform[-1].init_stats(
            num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2)
        )
        obs_norm_sd = test_env.transform[-1].state_dict()
        # let's check that normalizing constants have a size of ``[C, 1, 1]`` where
        # ``C=4`` (because of :class:`~torchrl.envs.CatFrames`).
        print("state dict of the observation norm:", obs_norm_sd)
        return obs_norm_sd









.. GENERATED FROM PYTHON SOURCE LINES 265-287

Building the model (Deep Q-network)
-----------------------------------

The following function builds a :class:`~torchrl.modules.DuelingCnnDQNet`
object which is a simple CNN followed by a two-layer MLP. The only trick used
here is that the action values (i.e. left and right action value) are
computed using

.. math::

   \mathbb{v} = b(obs) + v(obs) - \mathbb{E}[v(obs)]

where :math:`\mathbb{v}` is our vector of action values,
:math:`b` is a :math:`\mathbb{R}^n \rightarrow 1` function and :math:`v` is a
:math:`\mathbb{R}^n \rightarrow \mathbb{R}^m` function, for
:math:`n = \# obs` and :math:`m = \# actions`.

Our network is wrapped in a :class:`~torchrl.modules.QValueActor`,
which will read the state-action
values, pick up the one with the maximum value and write all those results
in the input :class:`tensordict.TensorDict`.


.. GENERATED FROM PYTHON SOURCE LINES 287-330

.. code-block:: default



    def make_model(dummy_env):
        cnn_kwargs = {
            "num_cells": [32, 64, 64],
            "kernel_sizes": [6, 4, 3],
            "strides": [2, 2, 1],
            "activation_class": nn.ELU,
            # This can be used to reduce the size of the last layer of the CNN
            # "squeeze_output": True,
            # "aggregator_class": nn.AdaptiveAvgPool2d,
            # "aggregator_kwargs": {"output_size": (1, 1)},
        }
        mlp_kwargs = {
            "depth": 2,
            "num_cells": [
                64,
                64,
            ],
            "activation_class": nn.ELU,
        }
        net = DuelingCnnDQNet(
            dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs
        ).to(device)
        net.value[-1].bias.data.fill_(init_bias)

        actor = QValueActor(net, in_keys=["pixels"], spec=dummy_env.action_spec).to(device)
        # init actor: because the model is composed of lazy conv/linear layers,
        # we must pass a fake batch of data through it to instantiate them.
        tensordict = dummy_env.fake_tensordict()
        actor(tensordict)

        # we wrap our actor in an EGreedyWrapper for data collection
        actor_explore = EGreedyWrapper(
            actor,
            annealing_num_steps=total_frames,
            eps_init=eps_greedy_val,
            eps_end=eps_greedy_val_env,
        )

        return actor, actor_explore









.. GENERATED FROM PYTHON SOURCE LINES 331-350

Collecting and storing data
---------------------------

Replay buffers
~~~~~~~~~~~~~~

Replay buffers play a central role in off-policy RL algorithms such as DQN.
They constitute the dataset we will be sampling from during training.

Here, we will use a regular sampling strategy, although a prioritized RB
could improve the performance significantly.

We place the storage on disk using
:class:`~torchrl.data.replay_buffers.storages.LazyMemmapStorage` class. This
storage is created in a lazy manner: it will only be instantiated once the
first batch of data is passed to it.

The only requirement of this storage is that the data passed to it at write
time must always have the same shape.

.. GENERATED FROM PYTHON SOURCE LINES 350-361

.. code-block:: default



    def get_replay_buffer(buffer_size, n_optim, batch_size):
        replay_buffer = TensorDictReplayBuffer(
            batch_size=batch_size,
            storage=LazyMemmapStorage(buffer_size),
            prefetch=n_optim,
        )
        return replay_buffer









.. GENERATED FROM PYTHON SOURCE LINES 362-390

Data collector
~~~~~~~~~~~~~~

As in `PPO <https://pytorch.org/rl/tutorials/coding_ppo.html>`_ and
`DDPG <https://pytorch.org/rl/tutorials/coding_ddpg.html>`_, we will be using
a data collector as a dataloader in the outer loop.

We choose the following configuration: we will be running a series of
parallel environments synchronously in parallel in different collectors,
themselves running in parallel but asynchronously.
The advantage of this configuration is that we can balance the amount of
compute that is executed in batch with what we want to be executed
asynchronously. We encourage the reader to experiment how the collection
speed is impacted by modifying the number of collectors (ie the number of
environment constructors passed to the collector) and the number of
environment executed in parallel in each collector (controlled by the
``num_workers`` hyperparameter).

When building the collector, we can choose on which device we want the
environment and policy to execute the operations through the ``device``
keyword argument. The ``storing_devices`` argument will modify the
location of the data being collected: if the batches that we are gathering
have a considerable size, we may want to store them on a different location
than the device where the computation is happening. For asynchronous data
collectors such as ours, different storing devices mean that the data that
we collect won't sit on the same device each time, which is something that
out training loop must account for. For simplicity, we set the devices to
the same value for all sub-collectors.

.. GENERATED FROM PYTHON SOURCE LINES 390-420

.. code-block:: default



    def get_collector(
        obs_norm_sd,
        num_collectors,
        actor_explore,
        frames_per_batch,
        total_frames,
        device,
    ):
        data_collector = MultiaSyncDataCollector(
            [
                make_env(parallel=True, obs_norm_sd=obs_norm_sd),
            ]
            * num_collectors,
            policy=actor_explore,
            frames_per_batch=frames_per_batch,
            total_frames=total_frames,
            # this is the default behaviour: the collector runs in ``"random"`` (or explorative) mode
            exploration_type=ExplorationType.RANDOM,
            # We set the all the devices to be identical. Below is an example of
            # heterogeneous devices
            device=device,
            storing_device=device,
            split_trajs=False,
            postproc=MultiStep(gamma=gamma, n_steps=5),
        )
        return data_collector









.. GENERATED FROM PYTHON SOURCE LINES 421-438

Loss function
-------------

Building our loss function is straightforward: we only need to provide
the model and a bunch of hyperparameters to the DQNLoss class.

Target parameters
~~~~~~~~~~~~~~~~~

Many off-policy RL algorithms use the concept of "target parameters" when it
comes to estimate the value of the next state or state-action pair.
The target parameters are lagged copies of the model parameters. Because
their predictions mismatch those of the current model configuration, they
help learning by putting a pessimistic bound on the value being estimated.
This is a powerful trick (known as "Double Q-Learning") that is ubiquitous
in similar algorithms.


.. GENERATED FROM PYTHON SOURCE LINES 438-447

.. code-block:: default



    def get_loss_module(actor, gamma):
        loss_module = DQNLoss(actor, delay_value=True)
        loss_module.make_value_estimator(gamma=gamma)
        target_updater = SoftUpdate(loss_module, eps=0.995)
        return loss_module, target_updater









.. GENERATED FROM PYTHON SOURCE LINES 448-454

Hyperparameters
---------------

Let's start with our hyperparameters. The following setting should work well
in practice, and the performance of the algorithm should hopefully not be
too sensitive to slight variations of these.

.. GENERATED FROM PYTHON SOURCE LINES 454-457

.. code-block:: default


    device = "cuda:0" if torch.cuda.device_count() > 0 else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 458-460

Optimizer
~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 460-470

.. code-block:: default


    # the learning rate of the optimizer
    lr = 2e-3
    # weight decay
    wd = 1e-5
    # the beta parameters of Adam
    betas = (0.9, 0.999)
    # Optimization steps per batch collected (aka UPD or updates per data)
    n_optim = 8








.. GENERATED FROM PYTHON SOURCE LINES 471-474

DQN parameters
~~~~~~~~~~~~~~
gamma decay factor

.. GENERATED FROM PYTHON SOURCE LINES 474-476

.. code-block:: default

    gamma = 0.99








.. GENERATED FROM PYTHON SOURCE LINES 477-480

Smooth target network update decay parameter.
This loosely corresponds to a 1/tau interval with hard target network
update

.. GENERATED FROM PYTHON SOURCE LINES 480-482

.. code-block:: default

    tau = 0.02








.. GENERATED FROM PYTHON SOURCE LINES 483-496

Data collection and replay buffer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::
  Values to be used for proper training have been commented.

Total frames collected in the environment. In other implementations, the
user defines a maximum number of episodes.
This is harder to do with our data collectors since they return batches
of N collected frames, where N is a constant.
However, one can easily get the same restriction on number of episodes by
breaking the training loop when a certain number
episodes has been collected.

.. GENERATED FROM PYTHON SOURCE LINES 496-498

.. code-block:: default

    total_frames = 5_000  # 500000








.. GENERATED FROM PYTHON SOURCE LINES 499-500

Random frames used to initialize the replay buffer.

.. GENERATED FROM PYTHON SOURCE LINES 500-502

.. code-block:: default

    init_random_frames = 100  # 1000








.. GENERATED FROM PYTHON SOURCE LINES 503-504

Frames in each batch collected.

.. GENERATED FROM PYTHON SOURCE LINES 504-506

.. code-block:: default

    frames_per_batch = 32  # 128








.. GENERATED FROM PYTHON SOURCE LINES 507-508

Frames sampled from the replay buffer at each optimization step

.. GENERATED FROM PYTHON SOURCE LINES 508-510

.. code-block:: default

    batch_size = 32  # 256








.. GENERATED FROM PYTHON SOURCE LINES 511-512

Size of the replay buffer in terms of frames

.. GENERATED FROM PYTHON SOURCE LINES 512-514

.. code-block:: default

    buffer_size = min(total_frames, 100000)








.. GENERATED FROM PYTHON SOURCE LINES 515-516

Number of environments run in parallel in each data collector

.. GENERATED FROM PYTHON SOURCE LINES 516-519

.. code-block:: default

    num_workers = 2  # 8
    num_collectors = 2  # 4








.. GENERATED FROM PYTHON SOURCE LINES 520-527

Environment and exploration
~~~~~~~~~~~~~~~~~~~~~~~~~~~

We set the initial and final value of the epsilon factor in Epsilon-greedy
exploration.
Since our policy is deterministic, exploration is crucial: without it, the
only source of randomness would be the environment reset.

.. GENERATED FROM PYTHON SOURCE LINES 527-531

.. code-block:: default


    eps_greedy_val = 0.1
    eps_greedy_val_env = 0.005








.. GENERATED FROM PYTHON SOURCE LINES 532-534

To speed up learning, we set the bias of the last layer of our value network
to a predefined value (this is not mandatory)

.. GENERATED FROM PYTHON SOURCE LINES 534-536

.. code-block:: default

    init_bias = 2.0








.. GENERATED FROM PYTHON SOURCE LINES 537-542

.. note::
  For fast rendering of the tutorial ``total_frames`` hyperparameter
  was set to a very low number. To get a reasonable performance, use a greater
  value e.g. 500000


.. GENERATED FROM PYTHON SOURCE LINES 544-561

Building a Trainer
------------------

TorchRL's :class:`~torchrl.trainers.Trainer` class constructor takes the
following keyword-only arguments:

- ``collector``
- ``loss_module``
- ``optimizer``
- ``logger``: A logger can be
- ``total_frames``: this parameter defines the lifespan of the trainer.
- ``frame_skip``: when a frame-skip is used, the collector must be made
  aware of it in order to accurately count the number of frames
  collected etc. Making the trainer aware of this parameter is not
  mandatory but helps to have a fairer comparison between settings where
  the total number of frames (budget) is fixed but the frame-skip is
  variable.

.. GENERATED FROM PYTHON SOURCE LINES 561-579

.. code-block:: default


    stats = get_norm_stats()
    test_env = make_env(parallel=False, obs_norm_sd=stats)
    # Get model
    actor, actor_explore = make_model(test_env)
    loss_module, target_net_updater = get_loss_module(actor, gamma)

    collector = get_collector(
        stats, num_collectors, actor_explore, frames_per_batch, total_frames, device
    )
    optimizer = torch.optim.Adam(
        loss_module.parameters(), lr=lr, weight_decay=wd, betas=betas
    )
    exp_name = f"dqn_exp_{uuid.uuid1()}"
    tmpdir = tempfile.TemporaryDirectory()
    logger = CSVLogger(exp_name=exp_name, log_dir=tmpdir.name)
    warnings.warn(f"log dir: {logger.experiment.log_dir}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    state dict of the observation norm: OrderedDict([('standard_normal', tensor(True)), ('loc', tensor([[[0.9895]],

            [[0.9895]],

            [[0.9895]],

            [[0.9895]]])), ('scale', tensor([[[0.0737]],

            [[0.0737]],

            [[0.0737]],

            [[0.0737]]]))])
    self.log_dir: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002




.. GENERATED FROM PYTHON SOURCE LINES 580-582

We can control how often the scalars should be logged. Here we set this
to a low value as our training loop is short:

.. GENERATED FROM PYTHON SOURCE LINES 582-596

.. code-block:: default


    log_interval = 500

    trainer = Trainer(
        collector=collector,
        total_frames=total_frames,
        frame_skip=1,
        loss_module=loss_module,
        optimizer=optimizer,
        logger=logger,
        optim_steps_per_batch=n_optim,
        log_interval=log_interval,
    )








.. GENERATED FROM PYTHON SOURCE LINES 597-608

Registering hooks
~~~~~~~~~~~~~~~~~

Registering hooks can be achieved in two separate ways:

- If the hook has it, the :meth:`~torchrl.trainers.TrainerHookBase.register`
  method is the first choice. One just needs to provide the trainer as input
  and the hook will be registered with a default name at a default location.
  For some hooks, the registration can be quite complex: :class:`~torchrl.trainers.ReplayBufferTrainer`
  requires 3 hooks (``extend``, ``sample`` and ``update_priority``) which
  can be cumbersome to implement.

.. GENERATED FROM PYTHON SOURCE LINES 608-628

.. code-block:: default

    buffer_hook = ReplayBufferTrainer(
        get_replay_buffer(buffer_size, n_optim, batch_size=batch_size),
        flatten_tensordicts=True,
    )
    buffer_hook.register(trainer)
    weight_updater = UpdateWeights(collector, update_weights_interval=1)
    weight_updater.register(trainer)
    recorder = Recorder(
        record_interval=100,  # log every 100 optimization steps
        record_frames=1000,  # maximum number of frames in the record
        frame_skip=1,
        policy_exploration=actor_explore,
        environment=test_env,
        exploration_type=ExplorationType.MODE,
        log_keys=[("next", "reward")],
        out_keys={("next", "reward"): "rewards"},
        log_pbar=True,
    )
    recorder.register(trainer)








.. GENERATED FROM PYTHON SOURCE LINES 629-637

- Any callable (including :class:`~torchrl.trainers.TrainerHookBase`
  subclasses) can be registered using :meth:`~torchrl.trainers.Trainer.register_op`.
  In this case, a location must be explicitly passed (). This method gives
  more control over the location of the hook but it also requires more
  understanding of the Trainer mechanism.
  Check the `trainer documentation <https://pytorch.org/rl/reference/trainers.html>`_
  for a detailed description of the trainer hooks.


.. GENERATED FROM PYTHON SOURCE LINES 637-639

.. code-block:: default

    trainer.register_op("post_optim", target_net_updater.step)








.. GENERATED FROM PYTHON SOURCE LINES 640-647

We can log the training rewards too. Note that this is of limited interest
with CartPole, as rewards are always 1. The discounted sum of rewards is
maximised not by getting higher rewards but by keeping the cart-pole alive
for longer.
This will be reflected by the `total_rewards` value displayed in the
progress bar.


.. GENERATED FROM PYTHON SOURCE LINES 647-650

.. code-block:: default

    log_reward = LogReward(log_pbar=True)
    log_reward.register(trainer)








.. GENERATED FROM PYTHON SOURCE LINES 651-660

.. note::
  It is possible to link multiple optimizers to the trainer if needed.
  In this case, each optimizer will be tied to a field in the loss
  dictionary.
  Check the :class:`~torchrl.trainers.OptimizerHook` to learn more.

Here we are, ready to train our algorithm! A simple call to
``trainer.train()`` and we'll be getting our results logged in.


.. GENERATED FROM PYTHON SOURCE LINES 660-662

.. code-block:: default

    trainer.train()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/5000 [00:00<?, ?it/s]      1%|          | 32/5000 [00:07<20:39,  4.01it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:   1%|          | 32/5000 [00:07<20:39,  4.01it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:   1%|▏         | 64/5000 [00:08<09:01,  9.11it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   1%|▏         | 64/5000 [00:08<09:01,  9.11it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   2%|▏         | 96/5000 [00:08<05:18, 15.38it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   2%|▏         | 96/5000 [00:08<05:18, 15.38it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   3%|▎         | 128/5000 [00:09<03:33, 22.80it/s]    r_training: 0.3620, rewards: 0.1000, total_rewards: 0.9434:   3%|▎         | 128/5000 [00:09<03:33, 22.80it/s]    r_training: 0.3620, rewards: 0.1000, total_rewards: 0.9434:   3%|▎         | 160/5000 [00:09<02:35, 31.10it/s]    r_training: 0.3923, rewards: 0.1000, total_rewards: 0.9434:   3%|▎         | 160/5000 [00:09<02:35, 31.10it/s]    r_training: 0.3923, rewards: 0.1000, total_rewards: 0.9434:   4%|▍         | 192/5000 [00:09<02:00, 39.75it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   4%|▍         | 192/5000 [00:09<02:00, 39.75it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   4%|▍         | 224/5000 [00:10<01:39, 48.01it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:   4%|▍         | 224/5000 [00:10<01:39, 48.01it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:   5%|▌         | 256/5000 [00:10<01:25, 55.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:   5%|▌         | 256/5000 [00:10<01:25, 55.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:   6%|▌         | 288/5000 [00:10<01:14, 63.11it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   6%|▌         | 288/5000 [00:10<01:14, 63.11it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   6%|▋         | 320/5000 [00:11<01:07, 68.98it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   6%|▋         | 320/5000 [00:11<01:07, 68.98it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:   7%|▋         | 352/5000 [00:11<01:03, 73.72it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:   7%|▋         | 352/5000 [00:11<01:03, 73.72it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:   8%|▊         | 384/5000 [00:12<01:00, 76.74it/s]    r_training: 0.4164, rewards: 0.1000, total_rewards: 0.9434:   8%|▊         | 384/5000 [00:12<01:00, 76.74it/s]    r_training: 0.4164, rewards: 0.1000, total_rewards: 0.9434:   8%|▊         | 416/5000 [00:12<00:57, 80.22it/s]    r_training: 0.3892, rewards: 0.1000, total_rewards: 0.9434:   8%|▊         | 416/5000 [00:12<00:57, 80.22it/s]    r_training: 0.3892, rewards: 0.1000, total_rewards: 0.9434:   9%|▉         | 448/5000 [00:12<00:55, 81.74it/s]    r_training: 0.3682, rewards: 0.1000, total_rewards: 0.9434:   9%|▉         | 448/5000 [00:12<00:55, 81.74it/s]    r_training: 0.3682, rewards: 0.1000, total_rewards: 0.9434:  10%|▉         | 480/5000 [00:13<00:53, 83.85it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  10%|▉         | 480/5000 [00:13<00:53, 83.85it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  10%|█         | 512/5000 [00:13<00:53, 84.15it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  10%|█         | 512/5000 [00:13<00:53, 84.15it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  11%|█         | 544/5000 [00:13<00:52, 84.76it/s]    r_training: 0.4134, rewards: 0.1000, total_rewards: 0.9434:  11%|█         | 544/5000 [00:13<00:52, 84.76it/s]    r_training: 0.4134, rewards: 0.1000, total_rewards: 0.9434:  12%|█▏        | 576/5000 [00:14<00:51, 85.64it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  12%|█▏        | 576/5000 [00:14<00:51, 85.64it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  12%|█▏        | 608/5000 [00:14<00:51, 85.27it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  12%|█▏        | 608/5000 [00:14<00:51, 85.27it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  13%|█▎        | 640/5000 [00:15<00:51, 83.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  13%|█▎        | 640/5000 [00:15<00:51, 83.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  13%|█▎        | 672/5000 [00:15<00:50, 84.94it/s]    r_training: 0.4373, rewards: 0.1000, total_rewards: 0.9434:  13%|█▎        | 672/5000 [00:15<00:50, 84.94it/s]    r_training: 0.4373, rewards: 0.1000, total_rewards: 0.9434:  14%|█▍        | 704/5000 [00:15<00:49, 86.37it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  14%|█▍        | 704/5000 [00:15<00:49, 86.37it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  15%|█▍        | 736/5000 [00:16<00:49, 85.77it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  15%|█▍        | 736/5000 [00:16<00:49, 85.77it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  15%|█▌        | 768/5000 [00:16<00:48, 87.70it/s]    r_training: 0.3712, rewards: 0.1000, total_rewards: 0.9434:  15%|█▌        | 768/5000 [00:16<00:48, 87.70it/s]    r_training: 0.3712, rewards: 0.1000, total_rewards: 0.9434:  16%|█▌        | 800/5000 [00:16<00:47, 89.03it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  16%|█▌        | 800/5000 [00:16<00:47, 89.03it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  17%|█▋        | 832/5000 [00:17<00:47, 88.08it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  17%|█▋        | 832/5000 [00:17<00:47, 88.08it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  17%|█▋        | 864/5000 [00:17<00:46, 88.95it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  17%|█▋        | 864/5000 [00:17<00:46, 88.95it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  18%|█▊        | 896/5000 [00:17<00:45, 89.54it/s]    r_training: 0.4524, rewards: 0.1000, total_rewards: 0.9434:  18%|█▊        | 896/5000 [00:17<00:45, 89.54it/s]    r_training: 0.4524, rewards: 0.1000, total_rewards: 0.9434:  19%|█▊        | 928/5000 [00:18<00:44, 90.55it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  19%|█▊        | 928/5000 [00:18<00:44, 90.55it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  19%|█▉        | 960/5000 [00:18<00:45, 88.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  19%|█▉        | 960/5000 [00:18<00:45, 88.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  20%|█▉        | 992/5000 [00:18<00:44, 89.94it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  20%|█▉        | 992/5000 [00:18<00:44, 89.94it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  20%|██        | 1024/5000 [00:19<00:44, 89.87it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  20%|██        | 1024/5000 [00:19<00:44, 89.87it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  21%|██        | 1056/5000 [00:19<00:43, 90.51it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  21%|██        | 1056/5000 [00:19<00:43, 90.51it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  22%|██▏       | 1088/5000 [00:20<00:43, 90.07it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  22%|██▏       | 1088/5000 [00:20<00:43, 90.07it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  22%|██▏       | 1120/5000 [00:20<00:42, 90.36it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  22%|██▏       | 1120/5000 [00:20<00:42, 90.36it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  23%|██▎       | 1152/5000 [00:20<00:42, 89.87it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  23%|██▎       | 1152/5000 [00:20<00:42, 89.87it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  24%|██▎       | 1184/5000 [00:21<00:42, 89.94it/s]    r_training: 0.4224, rewards: 0.1000, total_rewards: 0.9434:  24%|██▎       | 1184/5000 [00:21<00:42, 89.94it/s]    r_training: 0.4224, rewards: 0.1000, total_rewards: 0.9434:  24%|██▍       | 1216/5000 [00:21<00:42, 88.77it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  24%|██▍       | 1216/5000 [00:21<00:42, 88.77it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  25%|██▍       | 1248/5000 [00:21<00:42, 88.90it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  25%|██▍       | 1248/5000 [00:21<00:42, 88.90it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  26%|██▌       | 1280/5000 [00:22<00:42, 88.41it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  26%|██▌       | 1280/5000 [00:22<00:42, 88.41it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  26%|██▌       | 1312/5000 [00:22<00:41, 89.48it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:  26%|██▌       | 1312/5000 [00:22<00:41, 89.48it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:  27%|██▋       | 1344/5000 [00:22<00:41, 88.70it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  27%|██▋       | 1344/5000 [00:22<00:41, 88.70it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  28%|██▊       | 1376/5000 [00:23<00:40, 89.80it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  28%|██▊       | 1376/5000 [00:23<00:40, 89.80it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  28%|██▊       | 1408/5000 [00:23<00:40, 89.04it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  28%|██▊       | 1408/5000 [00:23<00:40, 89.04it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  29%|██▉       | 1440/5000 [00:23<00:39, 89.16it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  29%|██▉       | 1440/5000 [00:23<00:39, 89.16it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  29%|██▉       | 1472/5000 [00:24<00:39, 88.90it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  29%|██▉       | 1472/5000 [00:24<00:39, 88.90it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  30%|███       | 1504/5000 [00:24<00:38, 90.04it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  30%|███       | 1504/5000 [00:24<00:38, 90.04it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  31%|███       | 1536/5000 [00:25<00:38, 89.46it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  31%|███       | 1536/5000 [00:25<00:38, 89.46it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  31%|███▏      | 1568/5000 [00:25<00:38, 90.11it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  31%|███▏      | 1568/5000 [00:25<00:38, 90.11it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  32%|███▏      | 1600/5000 [00:25<00:37, 90.34it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  32%|███▏      | 1600/5000 [00:25<00:37, 90.34it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  33%|███▎      | 1632/5000 [00:26<00:37, 90.13it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  33%|███▎      | 1632/5000 [00:26<00:37, 90.13it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  33%|███▎      | 1664/5000 [00:26<00:37, 89.07it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  33%|███▎      | 1664/5000 [00:26<00:37, 89.07it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  34%|███▍      | 1696/5000 [00:26<00:37, 88.03it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  34%|███▍      | 1696/5000 [00:26<00:37, 88.03it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  35%|███▍      | 1728/5000 [00:27<00:36, 89.15it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  35%|███▍      | 1728/5000 [00:27<00:36, 89.15it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  35%|███▌      | 1760/5000 [00:27<00:36, 88.44it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  35%|███▌      | 1760/5000 [00:27<00:36, 88.44it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  36%|███▌      | 1792/5000 [00:27<00:35, 89.76it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  36%|███▌      | 1792/5000 [00:27<00:35, 89.76it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 0.9434:  36%|███▋      | 1824/5000 [00:28<00:35, 90.44it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  36%|███▋      | 1824/5000 [00:28<00:35, 90.44it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  37%|███▋      | 1856/5000 [00:28<00:34, 90.34it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  37%|███▋      | 1856/5000 [00:28<00:34, 90.34it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  38%|███▊      | 1888/5000 [00:28<00:34, 90.50it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  38%|███▊      | 1888/5000 [00:28<00:34, 90.50it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  38%|███▊      | 1920/5000 [00:29<00:34, 90.09it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  38%|███▊      | 1920/5000 [00:29<00:34, 90.09it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  39%|███▉      | 1952/5000 [00:29<00:33, 90.10it/s]    r_training: 0.4314, rewards: 0.1000, total_rewards: 0.9434:  39%|███▉      | 1952/5000 [00:29<00:33, 90.10it/s]    r_training: 0.4314, rewards: 0.1000, total_rewards: 0.9434:  40%|███▉      | 1984/5000 [00:30<00:33, 90.04it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:  40%|███▉      | 1984/5000 [00:30<00:33, 90.04it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:  40%|████      | 2016/5000 [00:30<00:33, 90.23it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  40%|████      | 2016/5000 [00:30<00:33, 90.23it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  41%|████      | 2048/5000 [00:30<00:32, 90.22it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  41%|████      | 2048/5000 [00:30<00:32, 90.22it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  42%|████▏     | 2080/5000 [00:31<00:32, 89.24it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  42%|████▏     | 2080/5000 [00:31<00:32, 89.24it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  42%|████▏     | 2112/5000 [00:31<00:32, 88.62it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  42%|████▏     | 2112/5000 [00:31<00:32, 88.62it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  43%|████▎     | 2144/5000 [00:31<00:31, 89.48it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  43%|████▎     | 2144/5000 [00:31<00:31, 89.48it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  44%|████▎     | 2176/5000 [00:32<00:31, 88.99it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  44%|████▎     | 2176/5000 [00:32<00:31, 88.99it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  44%|████▍     | 2208/5000 [00:32<00:31, 87.58it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  44%|████▍     | 2208/5000 [00:32<00:31, 87.58it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 0.9434:  45%|████▍     | 2240/5000 [00:32<00:31, 87.77it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  45%|████▍     | 2240/5000 [00:32<00:31, 87.77it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  45%|████▌     | 2272/5000 [00:33<00:30, 88.03it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  45%|████▌     | 2272/5000 [00:33<00:30, 88.03it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  46%|████▌     | 2304/5000 [00:33<00:30, 88.72it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:  46%|████▌     | 2304/5000 [00:33<00:30, 88.72it/s]    r_training: 0.4074, rewards: 0.1000, total_rewards: 0.9434:  47%|████▋     | 2336/5000 [00:34<00:30, 87.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  47%|████▋     | 2336/5000 [00:34<00:30, 87.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  47%|████▋     | 2368/5000 [00:34<00:29, 87.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  47%|████▋     | 2368/5000 [00:34<00:29, 87.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  48%|████▊     | 2400/5000 [00:34<00:29, 87.75it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  48%|████▊     | 2400/5000 [00:34<00:29, 87.75it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  49%|████▊     | 2432/5000 [00:35<00:29, 87.14it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  49%|████▊     | 2432/5000 [00:35<00:29, 87.14it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 0.9434:  49%|████▉     | 2464/5000 [00:35<00:29, 84.98it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  49%|████▉     | 2464/5000 [00:35<00:29, 84.98it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  50%|████▉     | 2496/5000 [00:35<00:30, 82.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  50%|████▉     | 2496/5000 [00:35<00:30, 82.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  51%|█████     | 2528/5000 [00:36<00:30, 82.40it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  51%|█████     | 2528/5000 [00:36<00:30, 82.40it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  51%|█████     | 2560/5000 [00:36<00:30, 80.69it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  51%|█████     | 2560/5000 [00:36<00:30, 80.69it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  52%|█████▏    | 2592/5000 [00:37<00:30, 78.86it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  52%|█████▏    | 2592/5000 [00:37<00:30, 78.86it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  52%|█████▏    | 2624/5000 [00:37<00:30, 78.67it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  52%|█████▏    | 2624/5000 [00:37<00:30, 78.67it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  53%|█████▎    | 2656/5000 [00:37<00:29, 80.63it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  53%|█████▎    | 2656/5000 [00:37<00:29, 80.63it/s]    r_training: 0.4526, rewards: 0.1000, total_rewards: 0.9434:  54%|█████▍    | 2688/5000 [00:38<00:27, 82.88it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  54%|█████▍    | 2688/5000 [00:38<00:27, 82.88it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  54%|█████▍    | 2720/5000 [00:38<00:27, 83.90it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  54%|█████▍    | 2720/5000 [00:38<00:27, 83.90it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  55%|█████▌    | 2752/5000 [00:39<00:26, 84.68it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  55%|█████▌    | 2752/5000 [00:39<00:26, 84.68it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  56%|█████▌    | 2784/5000 [00:39<00:26, 85.11it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  56%|█████▌    | 2784/5000 [00:39<00:26, 85.11it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  56%|█████▋    | 2816/5000 [00:39<00:25, 85.62it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  56%|█████▋    | 2816/5000 [00:39<00:25, 85.62it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  57%|█████▋    | 2848/5000 [00:40<00:24, 86.17it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  57%|█████▋    | 2848/5000 [00:40<00:24, 86.17it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 0.9434:  58%|█████▊    | 2880/5000 [00:40<00:24, 86.53it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  58%|█████▊    | 2880/5000 [00:40<00:24, 86.53it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  58%|█████▊    | 2912/5000 [00:40<00:23, 87.60it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  58%|█████▊    | 2912/5000 [00:40<00:23, 87.60it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  59%|█████▉    | 2944/5000 [00:41<00:23, 87.70it/s]    r_training: 0.4434, rewards: 0.1000, total_rewards: 0.9434:  59%|█████▉    | 2944/5000 [00:41<00:23, 87.70it/s]    r_training: 0.4434, rewards: 0.1000, total_rewards: 0.9434:  60%|█████▉    | 2976/5000 [00:41<00:23, 87.16it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  60%|█████▉    | 2976/5000 [00:41<00:23, 87.16it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  60%|██████    | 3008/5000 [00:42<00:22, 86.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  60%|██████    | 3008/5000 [00:42<00:22, 86.97it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  61%|██████    | 3040/5000 [00:42<00:22, 86.74it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  61%|██████    | 3040/5000 [00:42<00:22, 86.74it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  61%|██████▏   | 3072/5000 [00:42<00:22, 86.43it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  61%|██████▏   | 3072/5000 [00:42<00:22, 86.43it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  62%|██████▏   | 3104/5000 [00:43<00:21, 86.29it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  62%|██████▏   | 3104/5000 [00:43<00:21, 86.29it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 0.9434:  63%|██████▎   | 3136/5000 [00:43<00:21, 85.95it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  63%|██████▎   | 3136/5000 [00:43<00:21, 85.95it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  63%|██████▎   | 3168/5000 [00:43<00:21, 85.99it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  63%|██████▎   | 3168/5000 [00:43<00:21, 85.99it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 0.9434:  64%|██████▍   | 3200/5000 [00:44<00:20, 87.05it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  64%|██████▍   | 3200/5000 [00:44<00:20, 87.05it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 0.9434:  65%|██████▍   | 3232/5000 [00:51<02:13, 13.20it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  65%|██████▍   | 3232/5000 [00:51<02:13, 13.20it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  65%|██████▌   | 3264/5000 [00:51<01:38, 17.70it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  65%|██████▌   | 3264/5000 [00:51<01:38, 17.70it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  66%|██████▌   | 3296/5000 [00:52<01:13, 23.25it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  66%|██████▌   | 3296/5000 [00:52<01:13, 23.25it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  67%|██████▋   | 3328/5000 [00:52<00:55, 29.90it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  67%|██████▋   | 3328/5000 [00:52<00:55, 29.90it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  67%|██████▋   | 3360/5000 [00:52<00:43, 37.49it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  67%|██████▋   | 3360/5000 [00:52<00:43, 37.49it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  68%|██████▊   | 3392/5000 [00:53<00:35, 45.27it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  68%|██████▊   | 3392/5000 [00:53<00:35, 45.27it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  68%|██████▊   | 3424/5000 [00:53<00:29, 53.48it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  68%|██████▊   | 3424/5000 [00:53<00:29, 53.48it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  69%|██████▉   | 3456/5000 [00:53<00:25, 61.05it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  69%|██████▉   | 3456/5000 [00:53<00:25, 61.05it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  70%|██████▉   | 3488/5000 [00:54<00:22, 67.30it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 5.5556:  70%|██████▉   | 3488/5000 [00:54<00:22, 67.30it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 5.5556:  70%|███████   | 3520/5000 [00:54<00:20, 72.85it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  70%|███████   | 3520/5000 [00:54<00:20, 72.85it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  71%|███████   | 3552/5000 [00:55<00:18, 77.72it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  71%|███████   | 3552/5000 [00:55<00:18, 77.72it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  72%|███████▏  | 3584/5000 [00:55<00:17, 80.46it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  72%|███████▏  | 3584/5000 [00:55<00:17, 80.46it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  72%|███████▏  | 3616/5000 [00:55<00:16, 82.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  72%|███████▏  | 3616/5000 [00:55<00:16, 82.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  73%|███████▎  | 3648/5000 [00:56<00:16, 84.21it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  73%|███████▎  | 3648/5000 [00:56<00:16, 84.21it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  74%|███████▎  | 3680/5000 [00:56<00:15, 85.50it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  74%|███████▎  | 3680/5000 [00:56<00:15, 85.50it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  74%|███████▍  | 3712/5000 [00:56<00:14, 86.32it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 5.5556:  74%|███████▍  | 3712/5000 [00:56<00:14, 86.32it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 5.5556:  75%|███████▍  | 3744/5000 [00:57<00:14, 86.57it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  75%|███████▍  | 3744/5000 [00:57<00:14, 86.57it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  76%|███████▌  | 3776/5000 [00:57<00:14, 86.77it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  76%|███████▌  | 3776/5000 [00:57<00:14, 86.77it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  76%|███████▌  | 3808/5000 [00:57<00:13, 86.88it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  76%|███████▌  | 3808/5000 [00:57<00:13, 86.88it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  77%|███████▋  | 3840/5000 [00:58<00:13, 87.37it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  77%|███████▋  | 3840/5000 [00:58<00:13, 87.37it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  77%|███████▋  | 3872/5000 [00:58<00:12, 88.00it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 5.5556:  77%|███████▋  | 3872/5000 [00:58<00:12, 88.00it/s]    r_training: 0.4345, rewards: 0.1000, total_rewards: 5.5556:  78%|███████▊  | 3904/5000 [00:59<00:12, 87.58it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 5.5556:  78%|███████▊  | 3904/5000 [00:59<00:12, 87.58it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 5.5556:  79%|███████▊  | 3936/5000 [00:59<00:12, 88.17it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  79%|███████▊  | 3936/5000 [00:59<00:12, 88.17it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  79%|███████▉  | 3968/5000 [00:59<00:11, 88.33it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  79%|███████▉  | 3968/5000 [00:59<00:11, 88.33it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  80%|████████  | 4000/5000 [01:00<00:11, 87.85it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  80%|████████  | 4000/5000 [01:00<00:11, 87.85it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  81%|████████  | 4032/5000 [01:00<00:11, 87.74it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  81%|████████  | 4032/5000 [01:00<00:11, 87.74it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  81%|████████▏ | 4064/5000 [01:00<00:10, 88.56it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  81%|████████▏ | 4064/5000 [01:00<00:10, 88.56it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  82%|████████▏ | 4096/5000 [01:01<00:10, 89.39it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  82%|████████▏ | 4096/5000 [01:01<00:10, 89.39it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  83%|████████▎ | 4128/5000 [01:01<00:09, 88.87it/s]    r_training: 0.4224, rewards: 0.1000, total_rewards: 5.5556:  83%|████████▎ | 4128/5000 [01:01<00:09, 88.87it/s]    r_training: 0.4224, rewards: 0.1000, total_rewards: 5.5556:  83%|████████▎ | 4160/5000 [01:01<00:09, 88.07it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  83%|████████▎ | 4160/5000 [01:01<00:09, 88.07it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  84%|████████▍ | 4192/5000 [01:02<00:09, 88.79it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  84%|████████▍ | 4192/5000 [01:02<00:09, 88.79it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  84%|████████▍ | 4224/5000 [01:02<00:08, 88.19it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 5.5556:  84%|████████▍ | 4224/5000 [01:02<00:08, 88.19it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 5.5556:  85%|████████▌ | 4256/5000 [01:03<00:08, 87.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  85%|████████▌ | 4256/5000 [01:03<00:08, 87.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  86%|████████▌ | 4288/5000 [01:03<00:08, 87.73it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  86%|████████▌ | 4288/5000 [01:03<00:08, 87.73it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  86%|████████▋ | 4320/5000 [01:03<00:07, 87.45it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  86%|████████▋ | 4320/5000 [01:03<00:07, 87.45it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  87%|████████▋ | 4352/5000 [01:04<00:07, 88.00it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  87%|████████▋ | 4352/5000 [01:04<00:07, 88.00it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  88%|████████▊ | 4384/5000 [01:04<00:07, 87.78it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  88%|████████▊ | 4384/5000 [01:04<00:07, 87.78it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  88%|████████▊ | 4416/5000 [01:04<00:06, 88.69it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  88%|████████▊ | 4416/5000 [01:04<00:06, 88.69it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  89%|████████▉ | 4448/5000 [01:05<00:06, 88.69it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 5.5556:  89%|████████▉ | 4448/5000 [01:05<00:06, 88.69it/s]    r_training: 0.4586, rewards: 0.1000, total_rewards: 5.5556:  90%|████████▉ | 4480/5000 [01:05<00:05, 88.00it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  90%|████████▉ | 4480/5000 [01:05<00:05, 88.00it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  90%|█████████ | 4512/5000 [01:05<00:05, 88.58it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  90%|█████████ | 4512/5000 [01:05<00:05, 88.58it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  91%|█████████ | 4544/5000 [01:06<00:05, 88.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  91%|█████████ | 4544/5000 [01:06<00:05, 88.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  92%|█████████▏| 4576/5000 [01:06<00:04, 89.89it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  92%|█████████▏| 4576/5000 [01:06<00:04, 89.89it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  92%|█████████▏| 4608/5000 [01:06<00:04, 90.36it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  92%|█████████▏| 4608/5000 [01:06<00:04, 90.36it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  93%|█████████▎| 4640/5000 [01:07<00:03, 90.25it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 5.5556:  93%|█████████▎| 4640/5000 [01:07<00:03, 90.25it/s]    r_training: 0.4676, rewards: 0.1000, total_rewards: 5.5556:  93%|█████████▎| 4672/5000 [01:07<00:03, 90.40it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  93%|█████████▎| 4672/5000 [01:07<00:03, 90.40it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  94%|█████████▍| 4704/5000 [01:08<00:03, 90.62it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  94%|█████████▍| 4704/5000 [01:08<00:03, 90.62it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  95%|█████████▍| 4736/5000 [01:08<00:02, 89.37it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  95%|█████████▍| 4736/5000 [01:08<00:02, 89.37it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  95%|█████████▌| 4768/5000 [01:08<00:02, 85.88it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  95%|█████████▌| 4768/5000 [01:08<00:02, 85.88it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  96%|█████████▌| 4800/5000 [01:09<00:02, 81.73it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  96%|█████████▌| 4800/5000 [01:09<00:02, 81.73it/s]    r_training: 0.4497, rewards: 0.1000, total_rewards: 5.5556:  97%|█████████▋| 4832/5000 [01:09<00:02, 79.88it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 5.5556:  97%|█████████▋| 4832/5000 [01:09<00:02, 79.88it/s]    r_training: 0.4045, rewards: 0.1000, total_rewards: 5.5556:  97%|█████████▋| 4864/5000 [01:10<00:01, 78.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  97%|█████████▋| 4864/5000 [01:10<00:01, 78.80it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  98%|█████████▊| 4896/5000 [01:10<00:01, 80.27it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  98%|█████████▊| 4896/5000 [01:10<00:01, 80.27it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  99%|█████████▊| 4928/5000 [01:10<00:00, 82.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  99%|█████████▊| 4928/5000 [01:10<00:00, 82.78it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556:  99%|█████████▉| 4960/5000 [01:11<00:00, 84.13it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 5.5556:  99%|█████████▉| 4960/5000 [01:11<00:00, 84.13it/s]    r_training: 0.4797, rewards: 0.1000, total_rewards: 5.5556: 100%|█████████▉| 4992/5000 [01:11<00:00, 85.02it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556: 100%|█████████▉| 4992/5000 [01:11<00:00, 85.02it/s]    r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556: : 5024it [01:11, 86.85it/s]                            r_training: 0.4948, rewards: 0.1000, total_rewards: 5.5556: : 5024it [01:11, 86.85it/s]



.. GENERATED FROM PYTHON SOURCE LINES 663-664

We can now quickly check the CSVs with the results.

.. GENERATED FROM PYTHON SOURCE LINES 664-693

.. code-block:: default



    def print_csv_files_in_folder(folder_path):
        """
        Find all CSV files in a folder and prints the first 10 lines of each file.

        Args:
            folder_path (str): The relative path to the folder.

        """
        csv_files = []
        output_str = ""
        for dirpath, _, filenames in os.walk(folder_path):
            for file in filenames:
                if file.endswith(".csv"):
                    csv_files.append(os.path.join(dirpath, file))
        for csv_file in csv_files:
            output_str += f"File: {csv_file}\n"
            with open(csv_file, "r") as f:
                for i, line in enumerate(f):
                    if i == 10:
                        break
                    output_str += line.strip() + "\n"
            output_str += "\n"
        print(output_str)


    print_csv_files_in_folder(logger.experiment.log_dir)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    File: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002/scalars/r_training.csv
    512,0.4044681489467621
    1024,0.4044681489467621
    1536,0.4948333501815796
    2048,0.45262256264686584
    2560,0.44965070486068726
    3072,0.4948333501815796
    3584,0.4948333501815796
    4096,0.4948333501815796
    4608,0.4948333501815796

    File: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002/scalars/optim_steps.csv
    512,128.0
    1024,256.0
    1536,384.0
    2048,512.0
    2560,640.0
    3072,768.0
    3584,896.0
    4096,1024.0
    4608,1152.0

    File: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002/scalars/loss.csv
    512,0.3070302903652191
    1024,0.2062082290649414
    1536,0.3296405076980591
    2048,0.30849388241767883
    2560,0.3391888737678528
    3072,0.3545701801776886
    3584,0.3083878457546234
    4096,0.41265806555747986
    4608,0.632584810256958

    File: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002/scalars/grad_norm_0.csv
    512,2.759239673614502
    1024,2.0702359676361084
    1536,2.5036020278930664
    2048,3.549731731414795
    2560,3.1911914348602295
    3072,3.2671711444854736
    3584,3.158902168273926
    4096,5.887919902801514
    4608,6.081020832061768

    File: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002/scalars/rewards.csv
    3232,0.10000000894069672

    File: /tmp/tmpe29fgeeq/dqn_exp_a4653e30-5ee3-11ee-9053-0242ac120002/scalars/total_rewards.csv
    3232,5.55555534362793






.. GENERATED FROM PYTHON SOURCE LINES 694-716

Conclusion and possible improvements
------------------------------------

In this tutorial we have learned:

- How to write a Trainer, including building its components and registering
  them in the trainer;
- How to code a DQN algorithm, including how to create a policy that picks
  up the action with the highest value with
  :class:`~torchrl.modules.QValueNetwork`;
- How to build a multiprocessed data collector;

Possible improvements to this tutorial could include:

- A prioritized replay buffer could also be used. This will give a
  higher priority to samples that have the worst value accuracy.
  Learn more on the
  `replay buffer section <https://pytorch.org/rl/reference/data.html#composable-replay-buffers>`_
  of the documentation.
- A distributional loss (see :class:`~torchrl.objectives.DistributionalDQNLoss`
  for more information).
- More fancy exploration techniques, such as :class:`~torchrl.modules.NoisyLinear` layers and such.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 47.152 seconds)

**Estimated memory usage:**  661 MB


.. _sphx_glr_download_tutorials_coding_dqn.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: coding_dqn.py <coding_dqn.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: coding_dqn.ipynb <coding_dqn.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
